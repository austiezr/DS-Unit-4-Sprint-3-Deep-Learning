{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldr0HZ193GKb"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 1*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "# Neural Networks for Sequences (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Sequences (samples x time)\n",
      "x_train shape:  (25000, 80)\n",
      "x_test shape:  (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad Sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 87s 3ms/sample - loss: 0.1154 - accuracy: 0.9570 - val_loss: 0.5494 - val_accuracy: 0.8228\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 87s 3ms/sample - loss: 0.0855 - accuracy: 0.9706 - val_loss: 0.5936 - val_accuracy: 0.8174\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 89s 4ms/sample - loss: 0.0649 - accuracy: 0.9771 - val_loss: 0.7571 - val_accuracy: 0.8117\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 86s 3ms/sample - loss: 0.0487 - accuracy: 0.9844 - val_loss: 0.7499 - val_accuracy: 0.8058\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 89s 4ms/sample - loss: 0.0382 - accuracy: 0.9876 - val_loss: 0.8646 - val_accuracy: 0.8143\n"
     ]
    }
   ],
   "source": [
    "unicorns = model.fit(x_train, y_train,\n",
    "                     batch_size=batch_size, \n",
    "                     epochs=5, \n",
    "                     validation_data=(x_test,y_test),\n",
    "                     workers=-2,\n",
    "                     use_multiprocessing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(unicorns.history['loss'])\n",
    "plt.plot(unicorns.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "# LSTM Text generation with Keras (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are some recent headlines from schools around the country: In Indiana, officials played a segment of a 911 call of a teacher in a panic during the Columbine High School shooting to students. In Ohio, officers fired blank shots during an active-shooter drill. In South Carolina, an officer dressed in black posed as an intruder on an unannounced drill. In Michigan, a school is spending $48 million on a renovation that includes curved hallways and hiding niches, in hopes of protecting students from a mass shooting. In Florida, a police officer arrested two 6-year-old students for misdemeanor battery. In Colorado, teachers received buckets and kitty litter for students to use as toilets in case of a prolonged school lockdown.\\n\\nMass shootings, meaning incidents with at least two deaths, in schools are horrifying. But it is highly unlikely that a child would ever witness one. Research indicates that some security measures brought in to make schools safer — like realistic shooter trainings — may be causing children more harm than good.\\n\\nIt is 10 times more likely that a student will die on the way to school.\\n\\nOur chances of dying in a fire are also much greater — 1 in 1,500. But we don’t overreact.\\n\\nMore children have died from lightning strikes than from mass shootings in schools in the past 20 years. Still, we don’t obsess about them.\\n\\nExactly how common are school shootings?\\n\\nIn the two decades since Columbine, there have been 10 mass shootings in schools according to a recent analysis by James Alan Fox, a professor of criminology at Northeastern University who has been studying school violence for several decades. In total, 81 people have been killed, 64 of them students. That’s an average of four deaths per year, three of them students.\\n\\nEven one death is too many. But for perspective, 729 children committed suicide with a firearm in 2017, and 863 were victims of homicides by guns that year.\\n\\nSchool-age children killed by guns 729 suicides in 2017 863 homicides Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University. School-age children killed by guns 729 suicides in 2017 863 homicides Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University. School-age children killed by guns 729 suicides in 2017 863 homicides in 2017 Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University.\\n\\nNearly every public school in the country now conducts lockdown drills, and even the youngest students participate (last year, one school adapted a lullaby to prepare kindergartners). But very few studies have looked into the efficiency of these drills. One of them concluded that the practice can be helpful to teach students basic safety procedures. But to the author of the study, Jaclyn Schildkraut, an associate professor at the State University of New York at Oswego, there is no point in dramatizing the drills. “All that causes is fear,” she said.\\n\\nRestaurants have 10 times as many homicides as schools. Why do we want to arm teachers and not wait staffs?\\n\\n“There’s a misunderstanding in where the dangers are,” said Dewey G. Cornell, a psychologist and professor at the University of Virginia. “Kids are at far greater danger going to and from school, than they are in the classroom,” he said. “School counseling, academic support, that’s gonna do far more to keep our communities safe.”\\n\\nUnlike the United States, the other wealthy countries in the Group of Seven don’t do lockdown drills and rarely have school shootings. What is the United States doing that is so different from them?\\n\\nGun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017. Gun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017. Gun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017.\\n\\nMany researchers think easy access to guns is an important part of the problem. “Violence in schools is just a small part of the larger problem of gun violence in our society,” Cornell wrote in a statement about prevention of violence in schools and communities.\\n\\nMisguided safety measures, such as dramatized lockdown drills, may give us the impression that we are protecting children, when, in fact, we are handing them a burden that adults are failing to address.\\n\\nRead more:\\n\\n‘What if someone was shooting?’\\n\\nThey grew up practicing lockdown drills. Now they’re steering the conversation on gun violence.\\n\\nSchool shootings are extraordinarily rare. Why is fear of them driving policy?\\n\\nPutting more cops in schools won’t make schools safer, and it will likely inflict a lot of harm'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Gather all text \n",
    "# Why? 1. See all possible characters 2. For training / splitting later\n",
    "text = \" \".join(data)\n",
    "\n",
    "# Unique Characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  178374\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78,\n",
       " 61,\n",
       " 77,\n",
       " 41,\n",
       " 46,\n",
       " 94,\n",
       " 52,\n",
       " 11,\n",
       " 41,\n",
       " 94,\n",
       " 77,\n",
       " 88,\n",
       " 103,\n",
       " 76,\n",
       " 61,\n",
       " 74,\n",
       " 11,\n",
       " 116,\n",
       " 77,\n",
       " 94,\n",
       " 4,\n",
       " 41,\n",
       " 87,\n",
       " 87,\n",
       " 48,\n",
       " 53,\n",
       " 119,\n",
       " 103,\n",
       " 45,\n",
       " 61,\n",
       " 11,\n",
       " 4,\n",
       " 119,\n",
       " 103,\n",
       " 94,\n",
       " 4,\n",
       " 103,\n",
       " 61,\n",
       " 77,\n",
       " 103]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 121)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178374 samples\n",
      "Epoch 1/10\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.5352- ETA: 0s - loss:\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"ars could have a tough time earning a wi\"\n",
      "ars could have a tough time earning a witg ad sur to R, cuanddliowt. Gesnti, ther to natt uedlania gotid axeestpyoth in beed.\n",
      "\n",
      "Ssentriloo the wine letit, metrating his antedu:c.\n",
      "\n",
      "Thet forithto. Pappicigh Maane ther. and to Unluranes or ald bias pateniss Hleriabn uy elardewts wilisenting boselliging the plite’t ngprot, erideran that N.:0\n",
      "\n",
      "A.\n",
      "\n",
      "Throiy Urics.”\n",
      "\n",
      "Ssmptiden Praake Rers fun cleonlintse thets and aikerald Compritlingeenu -aspiti\n",
      "178374/178374 [==============================] - 156s 877us/sample - loss: 2.5349\n",
      "Epoch 2/10\n",
      "178336/178374 [============================>.] - ETA: 0s - loss: 2.1733\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"e.\n",
      "\n",
      "Trump crossed the 10,000 mark on Apr\"\n",
      "e.\n",
      "\n",
      "Trump crossed the 10,000 mark on Apritet polshes Squided on the Seremon for the h-— teatie dicafing to to of the U. 1. holR the H’end tere pamedels canuringatly the runows Mlllased porecl leste Wraserturmaton that thes seationA’s in and it be the’k howey. (3\n",
      "\n",
      "Af Eollas or warks mear dic aly (Recads of brasing to tI that cejation quachidal Treaghs and for serorded ty altory becan ully or le-lestit, at fory fore doted for to earls, su\n",
      "178374/178374 [==============================] - 164s 919us/sample - loss: 2.1732\n",
      "Epoch 3/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 2.0251\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"S feeds or any related activity.\n",
      "\n",
      "\n",
      "\n",
      "We m\"\n",
      "S feeds or any related activity.\n",
      "\n",
      "\n",
      "\n",
      "We moce that art wis 101 thin sear Trump’s nogg’s hile groot scoubs messids, and Sunder dessagrabions and Wele litsing to inal Ukrea moon yerro whet goute net ip oruct the stomed. Ralic, and the tith actueds reeved justen farm to in ifneralle the recomed to ther harr kin and her deach the dobly fom the Prom se peastiend cotffested to ghe Chart of the incule camcuased in undignt retime. Tur Prollisif (\n",
      "178374/178374 [==============================] - 167s 936us/sample - loss: 2.0251\n",
      "Epoch 4/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 1.9282\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"Kurt Vonnegut reading from his novel “Sl\"\n",
      "Kurt Vonnegut reading from his novel “Sluded to dogilising andngringy courtederemed mustion undurich to make by in gany vess-aded textethan bhat Syriansble and finday, which he’s cotnations it resto/lised with a bein and nook concell’s commttiny, trears hon they whose-maysed off it dumbicies no lica wast —” asted Allans Pession andey but notd of Harint has relate-tleesproge of thook be whitten. In menianarinate. Wory the lucces and shoo\n",
      "178374/178374 [==============================] - 168s 939us/sample - loss: 1.9282\n",
      "Epoch 5/10\n",
      "178336/178374 [============================>.] - ETA: 0s - loss: 1.8547\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"to recapture the depreciation you took w\"\n",
      "to recapture the depreciation you took worghtroping drewatis of 10 om ithly le grof dosing or fulling antitian for for jod and insucaine inverait or to dachres on a beat make brain “cales, an you his othor with the supborning for his co aitarity a crit cationating the remains of the later. In the hovernory of “Angrin Jo. (@BCLC-ON. statain meselt quarchoch me the Mighlined\n",
      "\n",
      "Ma.me womge discech to is laysonce knew sac marements of your p\n",
      "178374/178374 [==============================] - 167s 934us/sample - loss: 1.8547\n",
      "Epoch 6/10\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.7937\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"er attorney general Jeff Sessions or for\"\n",
      "er attorney general Jeff Sessions or for the with awcramberg motten, Mostusson not t2 mackenge in costsucted aliain persos for light as a prescep in Chiff has mic late Bodier ame. QSiste/K Tuuns uNia States humpuges nold nom Bo then, the flosed freengy finch loss foo flesirgest inccup and he to the istaks, Istrial prepically becriebsions diachite es on thi dout in Porray, States jo prote lige wheans, in a crantsed, bumne or 13, the $20.\n",
      "178374/178374 [==============================] - 164s 918us/sample - loss: 1.7937\n",
      "Epoch 7/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 1.7451\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \" agreement with Iran and other world pow\"\n",
      " agreement with Iran and other world pownat Probpheth deewned the readence Resollawman Bidin and Chila, brandles ot the beachdoblist's roll continuies.\n",
      "\n",
      "“Turre Washels Dain wher Iplumlank he had for a confertiel withering that considerity cothingly abous lust sweem.”\n",
      "\n",
      "This coume the 19, presidances optomen. Then relettion [bom the said. after sitcher at the bold the vinaly not least peachparteem the preserits of a spowaly, the fame come\n",
      "178374/178374 [==============================] - 169s 949us/sample - loss: 1.7451\n",
      "Epoch 8/10\n",
      "178336/178374 [============================>.] - ETA: 0s - loss: 1.7022\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"ceable, that portion shall be construed \"\n",
      "ceable, that portion shall be construed requents and duauld that the New Privy) — and something for senerats lightits foo keals-bublbers.\n",
      "\n",
      "“Rovell as our recass hurd, claincy. Hatisber. “We wele way parted comment popy jaure fire riles to has so a us thing vecy. In thif . And landss of the Washington to or pountes becaps. Buty Stabbea, first fom no the fendentive wolld arsing to inficioding the now years and when often or truandent thou\n",
      "178374/178374 [==============================] - 177s 992us/sample - loss: 1.7022\n",
      "Epoch 9/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 1.6647\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"pular vote.\n",
      "\n",
      "“I was investigated, okay? \"\n",
      "pular vote.\n",
      "\n",
      "“I was investigated, okay? Ellight of cairative.\n",
      "\n",
      "ADPH. chailes gow meppers day it thee man you millistd of Amext at the care the expoting at the copys taby tiall states.\n",
      "\n",
      "\n",
      "\n",
      "F.\n",
      "\n",
      "‘I hearing wam actise an leng U.S. his there “chrivist gating day awound part time his copired thinks by the D\n",
      "\n",
      "By polkini, that,” the strost my has he woued 17 vice freend Phirge’s persed for our stare and succory to batand clobert 30 plays bee in \n",
      "178374/178374 [==============================] - 164s 921us/sample - loss: 1.6647\n",
      "Epoch 10/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 1.6329\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"oers appear to be yelling at the preside\"\n",
      "oers appear to be yelling at the president condroum a down awal and enerity for a lank: Ane necenornction lacist team is not itient and munger in 19-hounder of 2 saver as a during the Turks on Arrise Chersh posici. Thisking on I have. (Falid.”\n",
      "\n",
      "AD\n",
      "\n",
      "S eleck oncading of Cilauser’s gradm. Afthore that herp.\n",
      "\n",
      "On Housh Prestrect Spriviotan list gors the whaught gamests panded on Spoce is a resalbarty in a flaces read will about a for around \n",
      "178374/178374 [==============================] - 207s 1ms/sample - loss: 1.6329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x146b40f10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use a Keras LSTM to generate text on today's assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "    * Sequence Problems:\n",
    "        - Time Series (like Stock Prices, Weather, etc.)\n",
    "        - Text Classification\n",
    "        - Text Generation\n",
    "        - And many more! :D\n",
    "    * LSTMs are generally preferred over RNNs for most problems\n",
    "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
    "    * Keras has LSTMs/RNN layer types implemented nicely\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
    "    * Shape of input data is very important\n",
    "    * Can take a while to train\n",
    "    * You can use it to write movie scripts. :P "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S3",
   "language": "python",
   "name": "ds-unit-4-sprint-3-deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
